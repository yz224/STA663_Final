{
 "metadata": {
  "name": "",
  "signature": "sha256:d7874232b3620578157d23fdc3759a3121bbd6818a1b2d58e25914fa74a544c7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Paper Choice: Bayesian Hierachical Clustering\n",
      "###Yikun Zhou (yz224)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. background:  \n",
      "Hierarchical clustering is one of the most frequently used methods in unsupervised learning.But the traditional hierarchical clustering method provides no guide to choosing the \u201ccorrect\u201d number of clusters or the level at which to prune the tree. Also, The traditional algorithm does not define a probabilistic model of the data, so it is hard to ask how \u201cgood\u201d a clustering is. So, here we can use Bayesian Hierachical Clustering to overcome these limitations. \n",
      "\n",
      "###Problem that BHC can solve:\n",
      "\n",
      "a). bayesian model can give probability of test point belonging to any existing cluster  \n",
      "b). merging cluster based on model rather than ad-hoc matrics  \n",
      "c). use hypothesis test to give recommendation of merges  \n",
      "d). can be intepreted as approximation of dirichlet process  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2.Outline Of Bayesian Hierachical Clustering\n",
      "Basic idea: initial each data point in its own clustering and iteratively merge pairs of clusters. Main difference from agglomerative algorithm is using hypothesis testing to make the merging decision"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Psedocode:\n",
      "\n",
      "####step1. \n",
      "input data $D=(x^1,x^2,...,x^n)$, define model  $p(x|\\theta)$, prior  $p(\\theta|\\beta)$\n",
      "\n",
      "####step2.\n",
      "initialize number of clusters c = n  \n",
      "each cluster$D_i = (x^i)$ for $i=1,2,..n$\n",
      "\n",
      "####step3.\n",
      "While c>1  \n",
      "find pair $D_i, D_j$with highest probability of merge:  \n",
      "  \n",
      "$r_k = \\dfrac{\\pi_k p(D_k|H_1^k)}{p(D_k|T_k)}$\n",
      "  \n",
      "Then Merge $D_i, D_j -> D_k$  \n",
      "$c <- c-1$\n",
      "\n",
      "end\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Initial Naive Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 30\n",
      "plt.style.use('ggplot')\n",
      "import random\n",
      "import itertools\n",
      "import math\n",
      "import scipy.stats as sps\n",
      "import numpy.linalg as npl\n",
      "import operator\n",
      "import pytest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####First, we need to set up some initial value for each points:  \n",
      "$$P(D_i)\\sim N(\\mu_0,\\sigma_0^2)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initial values\n",
      "mu0 = 0\n",
      "sigma0 = 1\n",
      "v0 = 1\n",
      "k0 = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Second, we write the function needed to calculate\n",
      "$$r_k=\\dfrac{\\pi_k P(D_k|H_1^k)}{\\pi_k P(D_k|H_1^k)+(1-\\pi_k) P(D_i|T_i)P(D_j|T_j)}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function used to calculate values in P(D|H_1)\n",
      "def P_DH(x,n,k0=1.0,v0=1.0,sigma0=1.0,mu0=0.0):\n",
      "    return (math.gamma((v0+n)/2)/math.gamma(v0/2))*np.sqrt(k0/(k0+n))*(((v0*sigma0)**(v0/2))/(((v0+n)*((1/(v0+n))*(v0*sigma0**2+sum((x-mu0)**2))**((v0+n)/2)))))*(1/math.pi**(n/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Third, Define some data structure to store the data needed in each step, including:  \n",
      "\n",
      ">Round: Store all the data needed to calculate r_k in each iteration,  type: Dictionary  \n",
      "DF: Store all the data points in each nodes, type: Dictionary  \n",
      "N: Store the number of data points of each nodes, type: Dictionary  \n",
      "D: Store hyperparameter d_k for each nodes, type: Dictionary\n",
      "PI: Store weights for each nodes, type: Dictionary  \n",
      "R: Store merging probability r_k for each possible group of Nodes, type: Dictionary"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initial Structure\n",
      "Rounds = {}\n",
      "Round = {}\n",
      "DF = {}\n",
      "N = {}\n",
      "D = {}\n",
      "R = {}\n",
      "x = np.array([1,1])\n",
      "\n",
      "# Basic Structure with data x\n",
      "for x in range(np.shape(x)[0]):\n",
      "    Round={'DF':DF, 'N':N, 'D':D, 'R':R}\n",
      "    Rounds['Round%s'%x] = Round"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Finally, we use algorithm defined before to calculate the result of BHC\n",
      "input data $D=(x^1,x^2,...,x^n)$, define model  $p(x|\\theta)$, prior  $p(\\theta|\\beta)$\n",
      "\n",
      "initialize number of clusters c = n  \n",
      "each cluster$D_i = (x^i)$ for $i=1,2,..n$\n",
      "\n",
      "While c>1  \n",
      "find pair $D_i, D_j$with highest probability of merge:  \n",
      "  \n",
      "$r_k = \\dfrac{\\pi_k p(D_k|H_1^k)}{p(D_k|T_k)}$\n",
      "  \n",
      "Then Merge $D_i, D_j -> D_k$  \n",
      "$c <- c-1$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define Initial BHC functions\n",
      "\n",
      "def Ini_BHC(df):\n",
      "\talpha0 = 0.5\n",
      "\tpi0 = 1\n",
      "\trang = range(np.shape(df)[0])\n",
      "\tmerge = {}\n",
      "\n",
      "\t# Set up initial values according to the data input and calculate priors\n",
      "\tfor i in range(np.shape(df)[0]):\n",
      "\t\tRounds['Round%s'%1]['DF%s'%i] = df[i,]\n",
      "\t\tRounds['Round%s'%1]['N%s'%i] = len(Rounds['Round%s'%1]['DF%s'%i])\n",
      "\t\tRounds['Round%s'%1]['D%s'%i] = alpha\n",
      "\t\tRounds['Round%s'%1]['R%s'%i] = P_DH(Rounds['Round%s'%1]['DF%s'%i],Rounds['Round%s'%1]['N%s'%i])\n",
      "    \n",
      "    # loop through all the data and store the merge situation\n",
      "\tfor r in range(1,np.shape(df)[0]):\n",
      "\t\tfor i in rang:\n",
      "\t\t\tstart_from = rang[rang.index(i)+1]\n",
      "\t\t\trangb = []\n",
      "\t\t\tfor val in rang[rang.index(start_from) if start_from  in rang else 1:]:\n",
      "\t\t\t\trangb.append(val)\n",
      "\t\t\t\trangb\n",
      "\t\t\tfor j in rangb:\n",
      "\t\t\t\tRounds['Round%s,%s'%(i,j)]['DF%s,%s'%(i,j)] = np.array([df[i,],df[j,]])\n",
      "\t\t\t\tRounds['Round%s,%s'%(i,j)]['N%s,%s'%(i,j)] = len(Rounds['Round%s,%s'%(i,j)]['DF%s,%s'%(i,j)])\n",
      "\t\t\t\tRounds['Round%s,%s'%(i,j)]['D%s,%s'%(i,j)] = alpha\n",
      "\t\t\t\tRounds['Round%s,%s'%(i,j)]['R%s,%s'%(i,j)] = P_DH(Rounds['Round%s,%s'%(i,j)]['DF%s,%s'%(i,j)],Rounds['Round%s,%s'%(i,j)]['N%s,%s'%(i,j)])\n",
      "\n",
      "    # Calculate merge probability using the data given above\n",
      "\tfor round in range(1,np.shape(df)[0]):\n",
      "\t\tr = {}\n",
      "\t    \n",
      "\t\tfor i in rang:\n",
      "\t\t\tif rang[rang.index(i)] != rang[-1]:\n",
      "\t\t\t\tstart_from = rang[rang.index(i)+1]\n",
      "\t\t\t\trangb = []\n",
      "\t\t\t\tfor val in rang[rang.index(start_from) if start_from  in rang else 1:]:\n",
      "\t\t\t\t\trangb.append(val)\n",
      "\t\t\t\t\trangb\n",
      "\t\t\t\tfor j in rangb:\n",
      "\t\t\t\t\tr[\"r%s,%s\" %(i,j)]=[Cluster(levels['level%s'%(round-1)]['cluster%s'%i],levels['level%s'%(round-1)]['cluster%s'%j]).r,'cluster%s'%i,'cluster%s'%j,i,j]\n",
      "        \n",
      "        # Store merge result\n",
      "\t\tmerge = max(r.iteritems(), key=operator.itemgetter(1))[1][1:5]\n",
      "\t    \n",
      "\t\tlevels['level%s'%round] = levels['level%s'%(round-1)].copy()\n",
      "\t\tfor key in max(r.iteritems(), key=operator.itemgetter(1))[1][1:3]:\n",
      "\t\t        if key in levels['level%s'%round]: del levels['level%s'%round][key]\n",
      "\n",
      "\t\tlevels['level%s'%round]['cluster%s'%(round+30)] = Cluster(levels['level%s'%(round-1)][merge[0]],levels['level%s'%(round-1)][merge[1]])\n",
      "        \n",
      "        # Re calculate the loop range\n",
      "\t\trang.remove(merge[2]) \n",
      "\t\trang.remove(merge[3])\n",
      "\t\trang.append(round+30)\n",
      "\n",
      "\t\tmerges['round%s'%round] = [merge,'cluster%s'%(round+30)]\n",
      "    \n",
      "\treturn merges"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Problem: The initial code above is OK and fast for 1 dimentional data. However, if I need to deal with data exceed 1 dimention, the speed will be very slow\n",
      "\n",
      "Possible Reasons:  \n",
      "* due to the function used to calculate $$r_k=\\dfrac{\\pi_k P(D_k|H_1^k)}{\\pi_k P(D_k|H_1^k)+(1-\\pi_k) P(D_i|T_i)P(D_j|T_j)}$$ is not vectorized.  \n",
      "* The data Structure needs to be optimized to simplify the calculatoin.   \n",
      "So next, I will profile and wirte a vectorized code which could better deal with high dimensional data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4. Vectorized Code: Optimize the code in both calculation speed and data structure\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####First, I designed a new function that can utilize vectorized data structure and calculate marginal likelihood faster\n",
      "The marginal likelihood is:\n",
      "$$P(D_k) = \\dfrac{1}{\\pi^{nd/2}} \\dfrac{\\Gamma_d(v_n/2)}{\\Gamma_d(v_0/2)}\\dfrac{\\Sigma_0^{v_0/2}}{\\Sigma_n^{v_n/2}}(\\dfrac{k_0}{k_n})^{d/2}$$  \n",
      "and we can use marginal likelihood to iteratively calculate the merging probability:\n",
      "$$r_k=\\dfrac{\\pi_k P(D_k|H_1^k)}{\\pi_k P(D_k|H_1^k)+(1-\\pi_k) P(D_i|T_i)P(D_j|T_j)}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function used to calculate values in P(D|H_1)\n",
      "def k_n(n,k0=1.0):\n",
      "    return k0+n\n",
      "\n",
      "def v_n(n,v0=1.0):\n",
      "    return v0+n\n",
      "\n",
      "def S(x,n):\n",
      "    return np.std(x,axis=0)*n\n",
      "\n",
      "def sigma_n(x,n,mu0,sigma0=np.matrix([[1.0,0.0],[.0,1.0]]),v0=1.0,k0=1.0):\n",
      "    return sigma0 + S(x,n) + (k0*n/(k0+n))*(np.dot(np.mean(x,axis=0)-mu0,np.transpose(np.mean(x,axis=0)-mu0)))\n",
      "\n",
      "# Function used to calculate value of P(D|H_1)\n",
      "def P_DH(x,n,mu0,sigma0=np.matrix([[1.0,0.0],[0.0,1.0]]),v0=1.0,k0=1.0,d=2.0):\n",
      "    return (math.gamma(v_n(n)/2.0)/math.gamma(v0/2.0))*((npl.det(sigma0)**(v0/2.0))/(npl.det(sigma_n(x,n,mu0))**(v_n(n)/2.0)))*(k0/k_n(n))**(d/2.0)*1/(math.pi)**(n*d/2.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Then, Design a better data structure to store the data needed in each step using Class object  \n",
      "\n",
      "He I designed two class object named ini_Cluster and Cluster  \n",
      "For ini_Cluster, its used for store the raw data and calculate some initial values  \n",
      "For Cluster, its used in the loop to calculate merging probability and mergings\n",
      "\n",
      "* For ini_Cluster object, it contains following properties:\n",
      ">.df: Store data point  \n",
      ".n: Store the number of data points of each nodes  \n",
      ".P: Store merging probability r_k for each data point  \n",
      ".d: Store hyperparameter d_k for each data point  \n",
      ".pi: Store weights for each data point  \n",
      "\n",
      "* For Cluster object, it contains following properties:\n",
      ">.cl: Store the left tree that construct the cluster  \n",
      ".cr: Store the right tree that construct the cluster  \n",
      ".df: Store all the data points in the cluster  \n",
      ".n: Store the number of data points in the cluster   \n",
      ".P: Store merging probability r_k for the cluster   \n",
      ".d: Store hyperparameter d_k for the cluster  \n",
      ".pi: Store weights for the cluster  \n",
      ".r: Store the merging probality of the cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ini_Cluster(object):\n",
      "\n",
      "    def __init__(self, n, df):\n",
      "        self.n  = n\n",
      "        self.df = df\n",
      "        self.P  = P_DH(self.df,self.n,mu0=self.df)\n",
      "        self.d  = alpha\n",
      "        self.pi = 1\n",
      "\n",
      "class Cluster(object):\n",
      "\n",
      "    def __init__(self, cl, cr):\n",
      "        self.cl = cl\n",
      "        self.cr = cr\n",
      "        self.n  = self.cl.n + self.cr.n\n",
      "        self.df = np.concatenate((self.cl.df,self.cr.df),axis=0)\n",
      "        self.P  = P_DH(self.df,self.n,mu0=np.mean(self.df,axis=0))\n",
      "        self.d  = alpha*math.gamma(self.n) + self.cl.d*self.cr.d\n",
      "        self.pi = alpha*math.gamma(self.n)/self.d\n",
      "        self.r  = self.pi*self.P/(self.pi*self.P+(1-self.pi)*self.cl.P*self.cr.P)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Finally, we use optimized algorithm defined before to calculate the result of BHC\n",
      "\n",
      "The basic algorithm is the same as before\n",
      ">input data $D=(x^1,x^2,...,x^n)$, define model  $p(x|\\theta)$, prior  $p(\\theta|\\beta)$\n",
      "initialize number of clusters c = n  \n",
      "each cluster$D_i = (x^i)$ for $i=1,2,..n$\n",
      "While c>1  \n",
      "find pair $D_i, D_j$with highest probability of merge:    \n",
      "$r_k = \\dfrac{\\pi_k p(D_k|H_1^k)}{p(D_k|T_k)}$\n",
      "Then Merge $D_i, D_j -> D_k$  \n",
      "$c <- c-1$\n",
      "\n",
      "* I used vectorized the function to calculate $r_k$ faster,  \n",
      "* I used better data structure to decreased the number of loops needed\n",
      "* The 'class' structure can calculate its property inside itself, which make the algorithm even faster\n",
      "* What's more, I defined $levels, merges, rang$ variables to make the algorithm more faster, see the result below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define the whole function BHC, the input is data that need to be clustered\n",
      "def BHC(df):\n",
      "    \n",
      "    # Define level varible to store all clusters in each iteration of calculation\n",
      "\tlevel0 = {}\n",
      "    \n",
      "    # First, assign value to the ini_Cluster object\n",
      "    # which will be used in latter calculation\n",
      "\tfor x in range(np.shape(df)[0]):\n",
      "\t    level0[\"cluster{0}\".format(x)]=ini_Cluster(1,np.array([df[x,]]))\n",
      "    \n",
      "    # Store initial clusters in 'level0' of levels variable\n",
      "\tlevels = {}\n",
      "\tlevels['level0'] = level0.copy()\n",
      "    \n",
      "    # Defined merge varible to store the merge decision of each iteration of calculation\n",
      "\tmerges = {}\n",
      "    \n",
      "    # Define rang for easier and faster looping \n",
      "\trang = range(np.shape(df)[0])\n",
      "    \n",
      "    # Calculate r_k for each group of data\n",
      "\tfor round in range(1,np.shape(df)[0]):\n",
      "\t\tr = {}\n",
      "\t\tfor i in rang:\n",
      "\t\t\tif rang[rang.index(i)] != rang[-1]:\n",
      "\t\t\t\tstart_from = rang[rang.index(i)+1]\n",
      "\t\t\t\trangb = []\n",
      "\t\t\t\tfor val in rang[rang.index(start_from) if start_from  in rang else 1:]:\n",
      "\t\t\t\t\trangb.append(val)\n",
      "\t\t\t\t\trangb\n",
      "\t\t\t\tfor j in rangb:\n",
      "\t\t\t\t\tr[\"r%s,%s\" %(i,j)]=[Cluster(levels['level%s'%(round-1)]['cluster%s'%i],levels['level%s'%(round-1)]['cluster%s'%j]).r,'cluster%s'%i,'cluster%s'%j,i,j]\n",
      "\n",
      "        # Store the result of merges\n",
      "\t\tmerge = max(r.iteritems(), key=operator.itemgetter(1))[1][1:5]\n",
      "\t    \n",
      "        # Delete merged clusters\n",
      "\t\tlevels['level%s'%round] = levels['level%s'%(round-1)].copy()\n",
      "\t\tfor key in max(r.iteritems(), key=operator.itemgetter(1))[1][1:3]:\n",
      "\t\t        if key in levels['level%s'%round]: del levels['level%s'%round][key]\n",
      "        \n",
      "        # Add new cluster merged to new iteration of calculation\n",
      "\t\tlevels['level%s'%round]['cluster%s'%(round+30)] = Cluster(levels['level%s'%(round-1)][merge[0]],levels['level%s'%(round-1)][merge[1]])\n",
      "\n",
      "        # modify rang variable to avoid redundant calculation\n",
      "\t\trang.remove(merge[2]) \n",
      "\t\trang.remove(merge[3])\n",
      "\t\trang.append(round+30)\n",
      "        \n",
      "        # Store result of each merge\n",
      "\t\tmerges['round%s'%round] = [merge,'cluster%s'%(round+30)]\n",
      "\treturn merges"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5.Application to a simulated question\n",
      "Next I will apply the code to a simulated dataset and see if the result will be show as expected"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Firstly, create Psedodata that could be used in clustering\n",
      "xs = np.random.normal(-1,1,5)\n",
      "xs = np.append(xs,np.random.normal(7,1,5))\n",
      "xs = np.append(xs,np.random.normal(20,2,5))\n",
      "ys = np.random.normal(10,2,5)\n",
      "ys = np.append(ys,np.random.normal(2,1,5))\n",
      "ys = np.append(ys,np.random.normal(8,1,5))\n",
      "\n",
      "df = pd.DataFrame(xs,columns=['x'])\n",
      "df['y'] = pd.Series(ys)\n",
      "df=np.array(df)\n",
      "plt.scatter(x=xs,y=ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<matplotlib.collections.PathCollection at 0x7f3b525c06d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEECAYAAADXg6SsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAcAghDoAGOT/AMfGqwGHBcQ7AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f3b5268d6d0>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BHC(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "{'round1': [['cluster6', 'cluster8', 6, 8], 'cluster31'],\n",
        " 'round10': [['cluster1', 'cluster2', 1, 2], 'cluster40'],\n",
        " 'round11': [['cluster33', 'cluster40', 33, 40], 'cluster41'],\n",
        " 'round12': [['cluster3', 'cluster41', 3, 41], 'cluster42'],\n",
        " 'round13': [['cluster39', 'cluster42', 39, 42], 'cluster43'],\n",
        " 'round14': [['cluster35', 'cluster43', 35, 43], 'cluster44'],\n",
        " 'round2': [['cluster5', 'cluster9', 5, 9], 'cluster32'],\n",
        " 'round3': [['cluster0', 'cluster4', 0, 4], 'cluster33'],\n",
        " 'round4': [['cluster31', 'cluster32', 31, 32], 'cluster34'],\n",
        " 'round5': [['cluster7', 'cluster34', 7, 34], 'cluster35'],\n",
        " 'round6': [['cluster10', 'cluster13', 10, 13], 'cluster36'],\n",
        " 'round7': [['cluster11', 'cluster36', 11, 36], 'cluster37'],\n",
        " 'round8': [['cluster12', 'cluster37', 12, 37], 'cluster38'],\n",
        " 'round9': [['cluster14', 'cluster38', 14, 38], 'cluster39']}"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!del p297-heller.pdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/bin/sh: 1: del: not found\r\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1_Descri_and_Algo.ipynb  Command.ipynb    README.md\r\n",
        "2_BHC_code.ipynb         Makefile.ipynb   test_clustering.py\r\n",
        "3_tests.ipynb            p297-heller.pdf  The_paper.ipynb\r\n",
        "4_Optimization.ipynb     \u001b[0m\u001b[01;34m__pycache__\u001b[0m/     tree.ipynb\r\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ls"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##6. Tests for code accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file test_clustering.py\n",
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import numpy as np\n",
      "import random\n",
      "import itertools\n",
      "import math\n",
      "import scipy.stats as sps\n",
      "import numpy.linalg as npl\n",
      "import operator\n",
      "import pytest\n",
      "\n",
      "# content of test_sample.py\n",
      "# Function used to calculate values in P(D|H_1)\n",
      "def k_n(n,k0=1.0):\n",
      "    return k0+n\n",
      "\n",
      "def v_n(n,v0=1.0):\n",
      "    return v0+n\n",
      "\n",
      "def S(x,n):\n",
      "    return np.std(x,axis=0)*n\n",
      "\n",
      "def sigma_n(x,n,mu0,sigma0=np.matrix([[1.0,0.0],[.0,1.0]]),v0=1.0,k0=1.0):\n",
      "    return sigma0 + S(x,n) + (k0*n/(k0+n))*(np.dot(np.mean(x,axis=0)-mu0,np.transpose(np.mean(x,axis=0)-mu0)))\n",
      "\n",
      "def P_DH(x,n,mu0,sigma0=np.matrix([[1.0,0.0],[0.0,1.0]]),v0=1.0,k0=1.0,d=2.0):\n",
      "    return (math.gamma(v_n(n)/2.0)/math.gamma(v0/2.0))*((npl.det(sigma0)**(v0/2.0))/(npl.det(sigma_n(x,n,mu0))**(v_n(n)/2.0)))*(k0/k_n(n))**(d/2.0)*1/(math.pi)**(n*d/2.0)\n",
      "\n",
      "# Test functions values\n",
      "def test_k_n():\n",
      "    assert k_n(5) == 6\n",
      "    \n",
      "def test_v_n():\n",
      "    assert v_n(3) == 4\n",
      "    \n",
      "def test_S():\n",
      "    test_data = np.array([[[1,0],[2,1]],[[2,2],[1,1]]])\n",
      "    result_value = np.array([[1,2],[1,0]])\n",
      "    assert npl.norm(S(test_data,2) - result_value)<= 0.001\n",
      "\n",
      "def test_PDH():\n",
      "    test_x = np.array([[[2,3],[2,1]],[[2,2],[1,1]]])\n",
      "    test_mu = np.array([1,1])\n",
      "    result_value = 0.006363984588752723270399513922 \n",
      "    assert abs(P_DH(test_x,2,test_mu) - result_value) < 0.0000001"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting test_clustering.py\n"
       ]
      }
     ],
     "prompt_number": 38
    }
   ],
   "metadata": {}
  }
 ]
}